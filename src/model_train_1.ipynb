{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for feature engineering and EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# library for statistic\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "from scipy.stats import boxcox, norm\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt\n",
    "\n",
    "# library for sampling\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "# library for machine learning\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import shap\n",
    "%matplotlib inline # Matplotlib의 시각화 결과를 노트북 내에서 바로 표시되도록 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datas/application_train.csv\", delimiter=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 data 확인 및 제거\n",
    "\n",
    "print(\"중복된 항목 수 :\", len(df[df.duplicated()]))\n",
    "\n",
    "has_duplicated = len(df[df.duplicated()]) != 0\n",
    "if (has_duplicated):\n",
    "  df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical and numeric columns\n",
    "for column_name in list(df.columns):\n",
    "    print(column_name, df[column_name].dtype, df[column_name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_categorical_columns = list(df.select_dtypes(include=['object']).columns)\n",
    "list_numeric_columns = list(df.select_dtypes(include=['float64','int64']).columns)\n",
    "target_column = \"TARGET\"\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(len(list_categorical_columns))\n",
    "print(len(list_numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Dependent(종속) Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_categorical_columns.remove(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_column].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=target_column, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Independent(독립) Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list_categorical_columns].nunique().sort_values()\n",
    "# 불필요한 컬럼 보이지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cateogircla column 별 분포 확인\n",
    "plt.figure(figsize=(15,30))\n",
    "x = 1\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.6, wspace=0.2)\n",
    "for column_name in list_categorical_columns:\n",
    "    plt.subplot(6,3,x)\n",
    "    x = x+1\n",
    "    df[column_name].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(column_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical column과 dependent data(target column) 분포 분석\n",
    "df_poutcome_dependent = pd.crosstab(df[target_column], df[list_categorical_columns[-1]])\n",
    "df_poutcome_dependent.plot(kind='bar')\n",
    "# 애초에 target_column(y) 비중이 다르기 때문에, 아래와 같이 count 수를 비교하는 것은 데이터 분포를 파악하는데 적합하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래와 같은 table을 출력\n",
    "pd.crosstab(df[target_column], df[list_categorical_columns[-1]], normalize=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poutcome_dependent_ratio = pd.crosstab(df[target_column], df[list_categorical_columns[-1]], normalize=\"index\")\n",
    "df_poutcome_dependent_ratio.plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이런 경우에는 비율 데이터를 plot으로 그리면 데이터 분포 이해에 도움.\n",
    "for column_name in list_categorical_columns:\n",
    "    pd.crosstab(df[target_column], df[column_name], normalize=\"index\").plot.bar()\n",
    "    plt.title(column_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카이제곱 검정 코드\n",
    "list_meaningful_column_by_chi = []\n",
    "\n",
    "for column_name in list_categorical_columns:\n",
    "  statistic, pvalue, _, _ = chi2_contingency(pd.crosstab(df[target_column], df[column_name]))\n",
    "  if pvalue <= 0.05:\n",
    "    list_meaningful_column_by_chi.append(column_name)\n",
    "  print(column_name, statistic, pvalue)\n",
    "\n",
    "print(\"all categorical columns : \", len(list_categorical_columns))\n",
    "print(\"selected columns by chi : \", len(list_meaningful_column_by_chi), list_meaningful_column_by_chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Numeric Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list_numeric_columns].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[list_numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric column 별 분포 확인\n",
    "plt.figure(figsize=(20,10))\n",
    "x = 1\n",
    "\n",
    "plt.subplots_adjust(top=0.99, bottom = 0.01, hspace = 0.4, wspace=0.2)\n",
    "for column_name in list_numeric_columns[:15]:\n",
    "  plt.subplot(4,4,x)\n",
    "  x = x + 1\n",
    "  sns.violinplot(x=column_name, data=df)\n",
    "  plt.title(column_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in list_numeric_columns:\n",
    "  print(column_name, \"skew : \", skew(df[column_name]), \"kur : \", kurtosis(df[column_name]) )\n",
    "\n",
    "# 추후 scaling을 활용한 feature preprocessing의 필요성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[list_numeric_columns].corr()\n",
    "plt.figure(figsize=(8,8))\n",
    "df_corr_for_view = df[list_numeric_columns[:15]].corr()\n",
    "sns.heatmap(df_corr_for_view, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 너무 높은 correlation을 갖는 데이터를 삭제. 단, 해당 correlation값을 신뢰할 수 있는지 확인필요\n",
    "# 기준은 절대값 0.75 이상\n",
    "index_corr_over_75 = np.where((abs(df_corr)>0.75) & (df_corr != 1))\n",
    "index_corr_over_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_corr_over_75 = len(index_corr_over_75[0])\n",
    "left_columns = df_corr.columns[index_corr_over_75[0]]\n",
    "right_columns = df_corr.columns[index_corr_over_75[1]]\n",
    "for index in range(len_corr_over_75):\n",
    "  print(left_columns[index], \"<->\", right_columns[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당하는 인덱스를 출력\n",
    "left_columns_over_75 = df_corr.columns[index_corr_over_75[0]]\n",
    "right_columns_over_75 = df_corr.columns[index_corr_over_75[1]]\n",
    "\n",
    "# 0.75 이상인 상관관계를 가진 열 리스트 출력\n",
    "over_75_columns = list(set(left_columns_over_75).union(set(right_columns_over_75)))\n",
    "print(len(over_75_columns), over_75_columns)\n",
    "\n",
    "list_removed_by_correlation = over_75_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "x = 1\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.2)\n",
    "for column_name in list_numeric_columns[:12]:\n",
    "    plt.subplot(4,3,x)\n",
    "    x = x + 1\n",
    "    sns.boxplot(data=df,x=target_column,y=column_name)\n",
    "plt.show()\n",
    "\n",
    "# 종속변수에 따른 previous 변수 분포 차이\n",
    "# 종속변수에 따른 duration 변수 부노 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric column 정규성 확인\n",
    "plt.figure(figsize=(18,10))\n",
    "x = 1\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)\n",
    "for column_name in list_numeric_columns[:12]:\n",
    "    plt.subplot(3,4,x)\n",
    "    x = x+1\n",
    "\n",
    "    stats.probplot(df[column_name], dist=stats.norm, plot=plt)\n",
    "\n",
    "    plt.title(column_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column_normality = []\n",
    "for column_name in list_numeric_columns:\n",
    "  statistic, pvalue = stats.shapiro(df[column_name])\n",
    "  if pvalue > 0.05:\n",
    "    list_column_normality.append(column_name)\n",
    "  print(column_name, \", statistic : \",statistic,\", pvalue : \", pvalue)\n",
    "print(\"정규성 만족하는 column 수 : \", len(list_column_normality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_meaningful_column_by_kruskall = []\n",
    "\n",
    "list_target_unique = df[target_column].unique()\n",
    "\n",
    "for column_name in list_numeric_columns:\n",
    "  list_by_target_value = []\n",
    "  for target_value in list_target_unique:\n",
    "    df_tmp = df[df[target_column]==target_value][column_name].dropna()\n",
    "    list_by_target_value.append(np.array(df_tmp))\n",
    "  statistic, pvalue = kruskal(*list_by_target_value)\n",
    "  if pvalue <= 0.05:\n",
    "    list_meaningful_column_by_kruskall.append(column_name)\n",
    "  print(column_name, \", \", statistic, \", \", pvalue)\n",
    "print(\"all numerical columns : \", len(list_numeric_columns))\n",
    "print(\"selected columns by kruskall : \", len(list_meaningful_column_by_kruskall), list_meaningful_column_by_kruskall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_selected_numerical = list_meaningful_column_by_kruskall.copy()\n",
    "for column_name in list_removed_by_correlation:\n",
    "  if (column_name in list_selected_numerical):\n",
    "    list_selected_numerical.remove(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs = df[list_meaningful_column_by_chi + list_selected_numerical]\n",
    "df_fs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_fs[target_column]\n",
    "X = df_fs.drop([target_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(Y)\n",
    "Y_encoded = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_categorical_columns = list(df_fs.select_dtypes(include=['object']).columns)\n",
    "list_numeric_columns = list(df_fs.select_dtypes(include=['float64','int64']).columns)\n",
    "print(len(list_categorical_columns))\n",
    "print(len(list_numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "list_numeric_columns = [col for col in list_numeric_columns if col != target_column]\n",
    "X.loc[:, list_numeric_columns] = scaler.fit_transform(X[list_numeric_columns])\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = pd.get_dummies(X)\n",
    "X_base.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X_base, Y_encoded, test_size=0.2, stratify=Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.create_experiment(\"bank_marketing_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"bank_marketing_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "n_split = 3\n",
    "skf = StratifiedKFold(n_splits=n_split)\n",
    "skf.get_n_splits(X_base, Y_encoded)\n",
    "\n",
    "list_fold_result_test = []\n",
    "list_fold_result_validation = []\n",
    "list_fold_roc_test = []\n",
    "list_fold_roc_validation = []\n",
    "num_hidden_layer_sizes = 10\n",
    "activation = 'relu'\n",
    "learning_rate_init = 0.001\n",
    "\n",
    "# mlflow.log_param(\"split num\", n_split)\n",
    "# mlflow.log_param(\"hidden_layer_sizes\", num_hidden_layer_sizes)\n",
    "# mlflow.log_param(\"activation\", activation)\n",
    "# mlflow.log_param(\"learning_rate_init\", learning_rate_init)\n",
    "# mlflow.log_param(\"sampling\", \"None\")\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"Split \" + str(i+1))\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_cv, y_test_cv = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # model set\n",
    "    rf_clf = MLPClassifier(hidden_layer_sizes=num_hidden_layer_sizes, activation=activation, learning_rate_init=learning_rate_init)\n",
    "    rf_clf.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    # model inference on test set\n",
    "    y_prd_test = rf_clf.predict(X_test_cv)\n",
    "    y_prd_proba_test = rf_clf.predict_proba(X_test_cv)[:,-1]\n",
    "\n",
    "    # score evaluation on test set\n",
    "    prf_score_test = precision_recall_fscore_support(y_test_cv, y_prd_test, average='macro')\n",
    "    score_test = roc_auc_score(y_test_cv, y_prd_proba_test, average='macro')\n",
    "    print(i, \" precision, recall, f1score : \", prf_score_test)\n",
    "    print(i, \" roc_auc_score : \", score_test)\n",
    "    list_fold_roc_test.append(score_test)\n",
    "    list_fold_result_test.append(prf_score_test)\n",
    "\n",
    "    # model inference on validation data set\n",
    "    y_prd_validation = rf_clf.predict(X_validation)\n",
    "    y_prd_proba_validation = rf_clf.predict_proba(X_validation)[:,-1]\n",
    "\n",
    "    # score evaluation on validation data set\n",
    "    prf_score_validation_macro = precision_recall_fscore_support(y_validation, y_prd_validation, average='macro')\n",
    "    score_validation = roc_auc_score(y_validation, y_prd_proba_validation, average='macro')\n",
    "    print(\"validation : precision, recall, f1score macro : \", prf_score_validation_macro)\n",
    "    print(\"validation : roc_auc_score : \", score_validation)\n",
    "    list_fold_roc_validation.append(score_validation)\n",
    "    list_fold_result_validation.append(prf_score_validation_macro)\n",
    "\n",
    "\n",
    "def get_prf_average(list_of_result):\n",
    "    pre = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    for result in list_fold_result_validation:\n",
    "        pre += result[0]\n",
    "        rec += result[1]\n",
    "        f1 += result[2]\n",
    "    return pre/n_split, rec/n_split, f1/n_split\n",
    "\n",
    "pre, rec, f1 = get_prf_average(list_fold_result_test)\n",
    "pre_val, rec_val, f1_val = get_prf_average(list_fold_result_validation)\n",
    "\n",
    "roc = sum(list_fold_roc_test)/n_split\n",
    "roc_val = sum(list_fold_roc_validation)/n_split\n",
    "\n",
    "# mlflow.log_metric(\"precision_on_test\", pre)\n",
    "# mlflow.log_metric(\"recall_on_test\", rec)\n",
    "# mlflow.log_metric(\"f1score_on_test\", f1)\n",
    "# mlflow.log_metric(\"roc_on_test\", roc)\n",
    "\n",
    "\n",
    "# mlflow.log_metric(\"precision_on_validation\", pre_val)\n",
    "# mlflow.log_metric(\"recall_on_validation\", rec_val)\n",
    "# mlflow.log_metric(\"f1score_on_validation\", f1_val)\n",
    "# mlflow.log_metric(\"roc_on_validation\", roc_val)\n",
    "\n",
    "\n",
    "def save_artifact(model, X_validation, y_validation, y_pred):\n",
    "    roc_plot = RocCurveDisplay.from_estimator(model,X_validation,y_validation,name='ML ROC CURVE')\n",
    "    plt.savefig('model_roc_plot.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "    ax=sns.heatmap(conf_matrix,annot=True,fmt='g',cmap='YlGnBu_r')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig('model_conf_matrix.png')\n",
    "    mlflow.log_artifact('model_roc_plot.png')\n",
    "    mlflow.log_artifact('model_conf_matrix.png')\n",
    "save_artifact(rf_clf, X_validation, y_validation, y_prd_validation)\n",
    "# mlflow.sklearn.log_model(rf_clf, \"model\")\n",
    "\n",
    "\n",
    "# mlflow.end_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
